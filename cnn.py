# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MDE254dg8cud50XZDb-B4Fp6HKnZM1K-

<center><h2>Importing required libraries</h2></center>
"""



from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import matplotlib.pyplot as plt
import cv2
import os
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ModelCheckpoint
from tqdm import tqdm

from tensorflow.keras.layers import BatchNormalization

from imutils import paths

import pickle

import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras import datasets, layers, models

from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer
from tensorflow.keras.models import Sequential
from keras import optimizers

from tensorflow.keras.models import load_model
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

"""<center><h2>Importing images</h2><center>"""

imagepaths = list(paths.list_images("../aug1"))
len(imagepaths)

data = []
labels = []
for images in tqdm(imagepaths):
    label = images.split(os.path.sep)[-2]
    image =cv2.imread(images)
    image =cv2.cvtColor(image,cv2.COLOR_BGR2RGB)
    image =cv2.resize(image, (50,50))
    image = np.asarray(image)
    data.append(image)
    labels.append(label)

imagepaths = list(paths.list_images("../../test"))
len(imagepaths)

t_data = []
t_labels = []
for images in tqdm(list(paths.list_images("../../test"))):
    label = images.split(os.path.sep)[-2]
    image =cv2.imread(images)
    image =cv2.cvtColor(image,cv2.COLOR_BGR2RGB)
    image =cv2.resize(image, (50,50))
    image = np.asarray(image)
    t_data.append(image)
    t_labels.append(label)

"""<center><h3>In case, have pickled data</h3></center>"""

traindata = pickle.load(open('/content/drive/MyDrive/ml/pickles/traindata.pkl','rb'))
trainlabels = pickle.load(open('/content/drive/MyDrive/ml/pickles/trainlabels.pkl','rb'))

testdata = pickle.load(open('/content/drive/MyDrive/ml/pickles/testdata.pkl', 'rb'))
testlabels = pickle.load(open('/content/drive/MyDrive/ml/pickles/testlabels.pkl', 'rb'))

itestdata = pickle.load(open('/content/drive/MyDrive/ml/final/itestdata.pkl', 'rb'))
itestlabels = pickle.load(open('/content/drive/MyDrive/ml/final/itestlabels.pkl', 'rb'))

"""<center><h3>In case, needed to pickle the data</h3></center>"""

# pickle.dump(traindata,open('newpickle/traindata.pkl','wb'))
# pickle.dump(trainlabels,open('newpickle/trainlabels.pkl','wb'))

# pickle.dump(testdata,open('newpickle/testdata.pkl','wb'))
# pickle.dump(testlabels,open('newpickle/testlabels.pkl','wb'))

"""<center><h3>Image data resizing</h3></center>"""

new_data = []
for img in data:
    img = np.reshape(img, (32,32,3))
    new_data.append(img)
data = new_data

new_data = []
for img in t_data:
    img = np.reshape(img, (32,32,3))
    new_data.append(img)
t_data = new_data



t_data = []
t_labels = []
for images in tqdm(list(paths.list_images("../../test"))):
    label = images.split(os.path.sep)[-2]
    image =cv2.imread(images)
    image =cv2.cvtColor(image,cv2.COLOR_BGR2RGB)
    image =cv2.resize(image, (50,50))
    image = np.asarray(image)
    t_data.append(image)
    t_labels.append(label)

"""<center><h3>Converting the string labels to integers</h3></center>"""

mapping = ['construction debris', 'e waste', 'green waste', 'medical waste', 'ocean waste', 'Papers _ cards', 'Plastics', 'recyclable waste', 'trash']

def index(x):
    for i in range(9):
        if(mapping[i] == x):
            return i;
    return -1;

ilabels = []
for l in trainlabels:
    ilabels.append(index(l))
trainlabels = ilabels
trainlabels = np.array(ilabels)

ilabels = []
for l in testlabels:
    ilabels.append(index(l))
testlabels = ilabels
testlabels = np.array(ilabels)

"""<center><h3>Converting the data array in range [0,1) - Normalising</h3></center>"""

def prep_pixels(train, test):
 	# convert from integers to floats
 	train_norm = np.array(train).astype('float32')
 	test_norm = np.array(test).astype('float32')
 	# normalize to range 0-1
 	train_norm = train_norm / 255.0
 	test_norm = test_norm/ 255.0
 	# return normalized images
 	return train_norm, test_norm

traindata, testdata = prep_pixels(traindata, testdata)

"""Just ref below"""

def prep_pixels(test):
 	# convert from integers to floats
 	test_norm = np.array(test).astype('float32')
 	# normalize to range 0-0
 	test_norm = test_norm/ 255.0
 	# return normalized images
 	return test_norm

testdata = prep_pixels(t_data)

"""<center><h3>Converting the labels to n-class vector</h3></center>"""

trainlabels = to_categorical(labels)
testlabels = to_categorical(t_labels)

traindata = data
testdata = t_data
trainlabels = labels
testlabels = t_labels

# testlabels = np.argmax(testlabels, axis = -1)
# trainlabels = np.argmax(trainlabels, axis = -1)





"""<center><h3>Converting all train and test data into np array</h3></center>"""

traindata = np.array(traindata)
trainlabels = np.array(trainlabels)
testdata = np.array(testdata)
testlabels = np.array(testlabels)

print(traindata.shape)
print(testdata.shape)
print(trainlabels.shape)
print(testlabels.shape)



"""<h2><center>4 layer CNN</center></h2>"""

model = models.Sequential()
model.add(layers.Conv2D(64, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
model.add(layers.Dropout(0.05))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
model.add(layers.Dropout(0.1))
model.add(layers.Flatten())
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])

model.summary()

#4 cnn layers
model = models.Sequential()
model.add(layers.Conv2D(64, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
model.add(layers.Dropout(0.05))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
model.add(layers.Dropout(0.1))
model.add(layers.Flatten())
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=15)
mc = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/4cnn.h5', monitor='val_accuracy', mode='max', verbose=2, save_best_only=True)

history = model.fit(traindata, trainlabels, validation_data=(testdata, testlabels),  epochs=400, verbose=2, callbacks=[es, mc])

from tensorflow.keras.models import load_model
saved_model = load_model('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/4cnn.h5')
# evaluate the model
model.save('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/4cnn.h5')
_, train_acc = model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))



print(history.history.keys())
#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

import pandas as pd
import h5py

hist_df = pd.DataFrame(history.history) 
hist_csv_file = '/content/drive/MyDrive/ml/csv_files/4cnn.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)



#4 cnn layers tune 1
model = models.Sequential()
model.add(layers.Conv2D(128, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(128,(3,3), activation = 'relu'))
model.add(layers.Dropout(0.05))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
model.add(layers.Dropout(0.1))
model.add(layers.Flatten())
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])
model.summary()

#4 cnn layers tune 1
model = models.Sequential()
model.add(layers.Conv2D(128, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(128,(3,3), activation = 'relu'))
model.add(layers.Dropout(0.05))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
model.add(layers.Dropout(0.1))
model.add(layers.Flatten())
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=15)
mc = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/4cnn_1.h5', monitor='val_accuracy', mode='max', verbose=2, save_best_only=True)

history = model.fit(traindata, trainlabels, validation_data=(testdata, testlabels),  epochs=400, verbose=2, callbacks=[es, mc])

from tensorflow.keras.models import load_model
saved_model = load_model('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/4cnn_1.h5')
# evaluate the model
model.save('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/4cnn_1.h5')
_, train_acc = model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))

print(history.history.keys())
#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

import pandas as pd
import h5py

hist_df = pd.DataFrame(history.history) 
hist_csv_file = '/content/drive/MyDrive/ml/csv_files/4cnn_1.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)

"""<center><h2>3 layers CNN</h2></center>"""

#3 layers
model = models.Sequential()
model.add(layers.Conv2D(32, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(30,(3,3), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.07))
model.add(layers.Conv2D(30,(3,3), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.15))
model.add(layers.Flatten())
model.add(layers.Dense(200,activation = 'relu'))
model.add(layers.Dropout(0.04))
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])
model.summary()

#8 layers
model = models.Sequential()
model.add(layers.Conv2D(32, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(30,(3,3), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.07))
model.add(layers.Conv2D(30,(3,3), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.15))
model.add(layers.Flatten())
model.add(layers.Dense(200,activation = 'relu'))
model.add(layers.Dropout(0.04))
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=15)
mc = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/best_model_8l_normal.h5', monitor='val_accuracy', mode='max', verbose=2, save_best_only=True)

history = model.fit(traindata, trainlabels, validation_data=(testdata, testlabels),  epochs=400, verbose=2, callbacks=[es, mc])

saved_model = load_model('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/best_model_8l_normal.h5')
# evaluate the model
_, train_acc = saved_model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = saved_model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))

saved_model = load_model('/content/drive/MyDrive/ml/final/cnn/models/cnn_8l_normal.h5')
# evaluate the model
_, train_acc = saved_model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = saved_model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))

print(history.history.keys())
#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

traindata, testdata, trainlabels, testlabels = train_test_split(traindata, trainlabels, random_state=32, test_size = 0.25)

import pandas as pd
import h5py

hist_df = pd.DataFrame(history.history) 
hist_csv_file = '/content/drive/MyDrive/Colab Notebooks/Garbage model/csv_files/cnn8l.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)



#3 layers - tuning 1
model = models.Sequential()
model.add(layers.Conv2D(40, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
# model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
# model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.07))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
# model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.15))
model.add(layers.Flatten())
model.add(layers.Dense(200,activation = 'relu'))
model.add(layers.Dropout(0.04))
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])
model.summary()

#3 layers - tuning 2
model = models.Sequential()
model.add(layers.Conv2D(40, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
# model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
# model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.07))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
# model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.15))
model.add(layers.Flatten())
model.add(layers.Dense(200,activation = 'relu'))
model.add(layers.Dropout(0.04))
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=15)
mc = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/best_model_8l_1.h5', monitor='val_accuracy', mode='max', verbose=2, save_best_only=True)

history = model.fit(traindata, trainlabels, validation_data=(testdata, testlabels),  epochs=400, verbose=2, callbacks=[es, mc])

saved_model = load_model('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/best_model_8l_1.h5')
# evaluate the model
_, train_acc = saved_model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = saved_model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))
model.save('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/cnn8l1.h5')

print(history.history.keys())
#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

import pandas as pd
import h5py

hist_df = pd.DataFrame(history.history) 
hist_csv_file = '/content/drive/MyDrive/Colab Notebooks/Garbage model/csv_files/cnn8l_1.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)

saved_model = load_model('/content/drive/MyDrive/ml/final/cnn/models/cnn8l1.h5')
# evaluate the model
_, train_acc = saved_model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = saved_model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))



#3 layers - tune 2
model = models.Sequential()
model.add(layers.Conv2D(50, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(50,(3,3), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.07))
model.add(layers.Conv2D(50,(3,3), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.15))
model.add(layers.Flatten())
model.add(layers.Dense(300,activation = 'relu'))
model.add(layers.Dropout(0.04))
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])
model.summary()

#3 layers - tune 2
model = models.Sequential()
model.add(layers.Conv2D(50, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(50,(3,3), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.07))
model.add(layers.Conv2D(50,(3,3), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.15))
model.add(layers.Flatten())
model.add(layers.Dense(300,activation = 'relu'))
model.add(layers.Dropout(0.04))
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=15)
mc = ModelCheckpoint('/content/drive/MyDrive/ml/models/best_model_8l_2.h5', monitor='val_accuracy', mode='max', verbose=2, save_best_only=True)

history = model.fit(traindata, trainlabels, validation_data=(testdata, testlabels),  epochs=400, verbose=2, callbacks=[es, mc])
model.save('/content/drive/MyDrive/ml/models/best_model_8l_2.h5')
saved_model = load_model('/content/drive/MyDrive/ml/models/best_model_8l_2.h5')
# evaluate the model
_, train_acc = saved_model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = saved_model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))

saved_model = load_model('/content/drive/MyDrive/ml/final/cnn/models/cnn_8l_2.h5')
# evaluate the model
_, train_acc = saved_model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = saved_model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))

print(history.history.keys())
#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()


import pandas as pd
import h5py

hist_df = pd.DataFrame(history.history) 
hist_csv_file = '/content/drive/MyDrive/ml/csv_files/cnn_8l_2.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)

"""<center><h2>2 layers CNN</h2></center>"""

#2 layers (normal)
model = models.Sequential()
model.add(layers.Conv2D(32, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(40,(3,3), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.15))
model.add(layers.Flatten())
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])
model.summary()

#2 layers (normal)
model = models.Sequential()
model.add(layers.Conv2D(32, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(40,(3,3), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.15))
model.add(layers.Flatten())
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=15)
mc = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/cnn_4l.h5', monitor='val_accuracy', mode='max', verbose=2, save_best_only=True)

history = model.fit(traindata, trainlabels, validation_data=(testdata, testlabels),  epochs=400, verbose=2, callbacks=[es, mc])

from tensorflow.keras.models import load_model
saved_model = load_model('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/cnn_4l.h5')
# evaluate the model
model.save('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/cnn_4l.h5')
_, train_acc = model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))

model2 = load_model('/content/drive/MyDrive/ml/final/cnn/models/cnn_4l.h5')
_, train_acc = model2.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = model2.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))

print(history.history.keys())
#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()


import pandas as pd
import h5py

hist_df = pd.DataFrame(history.history) 
hist_csv_file = '/content/drive/MyDrive/Colab Notebooks/Garbage model/csv_files/cnn_4l.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)

#2 layers - tuning 1
model = models.Sequential()
model.add(layers.Conv2D(32, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.15))
model.add(layers.Flatten())
model.add(layers.Dense(200,activation = 'relu'))
model.add(layers.Dropout(0.04))
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])
model.summary()

#2 layers - tuning 1 (*)
model = models.Sequential()
model.add(layers.Conv2D(32, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.15))
model.add(layers.Flatten())
model.add(layers.Dense(200,activation = 'relu'))
model.add(layers.Dropout(0.04))
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=15)
mc = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/best_model_6l_normal.h5', monitor='val_accuracy', mode='max', verbose=2, save_best_only=True)

history_embedding = model.fit(traindata, trainlabels, validation_data=(testdata, testlabels),  epochs=400, verbose=2, callbacks=[es, mc])

from tensorflow.keras.models import load_model
saved_model = load_model('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/best_model_6l_normal.h5')

# evaluate the model
_, train_acc = saved_model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = saved_model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))
model.save('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/cnn6l.h5')

# import matplotlib.pyplot as plt
# plt.plot(history_embedding.history['accuracy'],c='b',label='train accuracy')
# plt.plot(history_embedding.history['val_accuracy'],c='r',label='validation accuracy')
# plt.legend(loc='lower right')
# plt.show()

saved_model = load_model('/content/drive/MyDrive/ml/final/cnn/models/best_model_6l_normal.h5')

# evaluate the model
_, train_acc = saved_model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = saved_model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))

history = history_embedding
print(history.history.keys())
#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

import pandas as pd
import h5py

hist_df = pd.DataFrame(history_embedding.history) 
hist_csv_file = '/content/drive/MyDrive/Colab Notebooks/Garbage model/csv_files/cnn6l_normal.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)















"""<h4>New dataset</h4>"""

saved_model = load_model('/content/best_model_8l_normal.h5')
img = "test9.jpg"

image =cv2.imread(img)
image =cv2.cvtColor(image,cv2.COLOR_BGR2RGB)
testimg =cv2.resize(image, (32,32))
testimg = np.asarray(testimg)
import matplotlib.pyplot as plt
plt.imshow(testimg)
testimg = np.array(testimg).astype('float32')
testimg = testimg/ 255.0

testing = []
testing.append(testimg)
testing = np.array(testing)

pred = saved_model.predict(testing)

res = 0
max = -1
for i in range(9):
  if pred[0][i] > max:
    max = pred[0][i]
    res = i

print(mapping[res])

#Generate predictions with the model using our X values
y_pred_arr = saved_model.predict(testdata)

y_pred = []
def intonum(pred):
  res = 0
  max = -1
  for i in range(9):
    if pred[i] > max:
      max = pred[i]
      res = i
  return res

for i in y_pred_arr:
  y_pred.append(intonum(i))

y_pred_arr = np.asarray(y_pred_arr)

y_true = testlabels
#Get the confusion matrix
cf_matrix = confusion_matrix(y_true, y_pred)
print(cf_matrix)

testdata.shape

def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):

    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    #Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

#print(cm)

    fig, ax = plt.subplots(figsize=(7,7))
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')


    #Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")
    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax

np.set_printoptions(precision=2)

#Plotting the confusion matrix
confusion_mtx = confusion_matrix(y_true, y_pred)

#Defining the class labels
class_names=['construction debris', 'e waste', 'green waste', 'medical waste', 'ocean waste', 'Papers _ cards', 'Plastics', 'recyclable waste', 'trash']
# Plotting non-normalized confusion matrix
plot_confusion_matrix(y_true, y_pred, classes = class_names, title='Confusion matrix, without normalization')

import cv2
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
import matplotlib.pyplot as plt

i = 47

for i in range(400, 500):
    val = "i: "+str(i)+" True: "+str(mapping[y_true[i]])+"  Predicted: "+str(mapping[y_pred[i]])
    plt.figure()
    plt.title(val)
    imgplot = plt.imshow(testdata[i])

"""<h3>Pretraining (Transfer learning)</h3>

<center><h2>3 CNN layers preCNN</center>
"""

# 8 layers pretraining
model8pre = models.load_model('/content/drive/MyDrive/ml/models/best_model_8l_2.h5')
model8pre.pop()
model8pre.summary()

model = Sequential()
model.add(model8pre)
model.add(Flatten())
model.add(Dense(500, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(9, activation='sigmoid'))

model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])
model.summary()

# 8 layers pretraining
model8pre = models.load_model('/content/drive/MyDrive/ml/models/best_model_8l_2.h5')
model8pre.pop()
# model8pre.summary()

model = Sequential()
model.add(model8pre)
model.add(Flatten())
model.add(Dense(500, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(9, activation='sigmoid'))

model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])
model.summary()

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)
mc = ModelCheckpoint('/content/drive/MyDrive/ml/models/cnnpre_8l.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)

# Splitted dataset
history=model.fit(traindata, trainlabels, validation_data=(testdata, testlabels), epochs=200, verbose=2, callbacks=[es, mc])

print(history.history.keys())
#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()


import pandas as pd
import h5py

hist_df = pd.DataFrame(history.history) 
hist_csv_file = '/content/drive/MyDrive/ml/csv_files/cnn_pre_8l.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)

saved_model = models.load_model('/content/drive/MyDrive/ml/final/cnn/models/pretrained/cnnpre_8l.h5')
# evaluate the model
_, train_acc = saved_model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = saved_model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))

#Generate predictions with the model using our X values
y_pred_arr = saved_model.predict(testdata)

y_pred = []
def intonum(pred):
  res = 0
  max = -1
  for i in range(9):
    if pred[i] > max:
      max = pred[i]
      res = i
  return res

for i in y_pred_arr:
  y_pred.append(intonum(i))

y_pred_arr = np.asarray(y_pred_arr)

y_true = testlabels
#Get the confusion matrix
cf_matrix = confusion_matrix(y_true, y_pred)
print(cf_matrix)

def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):

    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    #Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

#print(cm)

    fig, ax = plt.subplots(figsize=(7,7))
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')


    #Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")
    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax

np.set_printoptions(precision=2)

#Plotting the confusion matrix
confusion_mtx = confusion_matrix(y_true, y_pred)

#Defining the class labels
class_names=['construction debris', 'e waste', 'green waste', 'medical waste', 'ocean waste', 'Papers _ cards', 'Plastics', 'recyclable waste', 'trash']
# Plotting non-normalized confusion matrix
plot_confusion_matrix(y_true, y_pred, classes = class_names, title='Confusion matrix, with normalization')



"""<h2><center>Inclass classification</center></h2>"""

def prep_pixels(test):
 	# convert from integers to floats
 	test_norm = np.array(test).astype('float32')
 	# normalize to range 0-0
 	test_norm = test_norm/ 255.0
 	# return normalized images
 	return test_norm

itestdata = prep_pixels(itestdata)

itestdata.shape

ilabels = []
for l in itestlabels:
    ilabels.append(index(l))
itestlabels = ilabels
itestlabels = np.array(itestlabels)

saved_model = models.load_model('/content/drive/MyDrive/ml/final/cnn/models/pretrained/cnnpre_8l.h5')
# evaluate the model
_, test_acc = saved_model.evaluate(itestdata, itestlabels, verbose=2)
print('Inclass Test Accuracy: %.3f' % (test_acc))























"""<h2><center>Other models (miscellenous)</center></h2>

<center><h2>6 layers preCNN</h2></center>
"""

# 6 layers pretraining
model6pre = models.load_model('/content/drive/MyDrive/ml/models/cnn6l.h5')
model6pre.pop()
# model6pre.summary()

model = Sequential()
model.add(model6pre)
model.add(Flatten())
model.add(Dense(500, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(9, activation='sigmoid'))

model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])
model.summary()

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)
mc = ModelCheckpoint('/content/drive/MyDrive/ml/models/cnnpre_6l_2.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)

# Splitted dataset
history=model.fit(traindata, trainlabels, validation_data=(testdata, testlabels), epochs=100, verbose=2, callbacks=[es, mc])

print(history.history.keys())
#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

import pandas as pd
import h5py

hist_df = pd.DataFrame(history.history) 
hist_csv_file = '/content/drive/MyDrive/Colab Notebooks/Garbage model/csv_files/cnn6l_1.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)

"""<center><h2>4 layers CNN</h2></center>"""



#6 layers - tuning 1
model = models.Sequential()
model.add(layers.Conv2D(20, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.03))
model.add(layers.Conv2D(20,(3,3), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.01))
model.add(layers.Flatten())
model.add(layers.Dense(200,activation = 'relu'))
model.add(layers.Dropout(0.08))
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)
mc = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/cnn6l_1.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)

history = model.fit(traindata, trainlabels, validation_data=(testdata, testlabels),  epochs=400, verbose=2, callbacks=[es, mc])

from tensorflow.keras.models import load_model
saved_model = load_model('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/cnn6l_1.h5')
# evaluate the model
_, train_acc = saved_model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = saved_model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))
model.save('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/cnn6l_1.h5')

#4 layers - tune 1
model = models.Sequential()
model.add(layers.Conv2D(16, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.1))
model.add(layers.Conv2D(16,(3,3), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Flatten())
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=15)
mc = ModelCheckpoint('/content/drive/MyDrive/ml/models/cnn_4l_1.h5', monitor='val_accuracy', mode='max', verbose=2, save_best_only=True)

history = model.fit(traindata, trainlabels, validation_data=(testdata, testlabels),  epochs=400, verbose=2, callbacks=[es, mc])
model.save('/content/drive/MyDrive/ml/models/cnn_4l_1.h5')

from tensorflow.keras.models import load_model
saved_model = load_model('/content/drive/MyDrive/ml/models/cnn_4l_1.h5')
# evaluate the model

_, train_acc = model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))

print(history.history.keys())
#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()


import pandas as pd
import h5py

hist_df = pd.DataFrame(history.history) 
hist_csv_file = '/content/drive/MyDrive/ml/csv_files/cnn_4l.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)



#4 layers - tune 2
model = models.Sequential()
model.add(layers.Conv2D(40, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Conv2D(40,(3,3), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Flatten())
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=15)
mc = ModelCheckpoint('/content/drive/MyDrive/ml/models/cnn_4l_2.h5', monitor='val_accuracy', mode='max', verbose=2, save_best_only=True)

history = model.fit(traindata, trainlabels, validation_data=(testdata, testlabels),  epochs=400, verbose=2, callbacks=[es, mc])
model.save('/content/drive/MyDrive/ml/models/cnn_4l_2.h5')

from tensorflow.keras.models import load_model
saved_model = load_model('/content/drive/MyDrive/ml/models/cnn_4l_2.h5')
# evaluate the model

_, train_acc = model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))

print(history.history.keys())
#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()


import pandas as pd
import h5py

hist_df = pd.DataFrame(history.history) 
hist_csv_file = '/content/drive/MyDrive/ml/csv_files/cnn_4l_2.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)

print(history.history.keys())
#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()


import pandas as pd
import h5py

hist_df = pd.DataFrame(history.history) 
hist_csv_file = '/content/drive/MyDrive/ml/csv_files/cnn_pre_6l.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)

_, train_acc = model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))

# 4 layers pretraining
model4pre = models.load_model('/content/drive/MyDrive/ml/models/cnn_4l_2.h5')
model4pre.pop()

model = Sequential()
model.add(model6pre)
model.add(Flatten())
model.add(Dense(200, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(9, activation='sigmoid'))

model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])
model.summary()

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)
mc = ModelCheckpoint('/content/drive/MyDrive/ml/models/cnnpre_4l_2.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)

# Splitted dataset
history=model.fit(traindata, trainlabels, validation_data=(testdata, testlabels), epochs=400, verbose=2, callbacks=[es, mc])

print(history.history.keys())
#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()


import pandas as pd
import h5py

hist_df = pd.DataFrame(history.history) 
hist_csv_file = '/content/drive/MyDrive/ml/csv_files/cnn_pre_4l.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)

saved_model = models.load_model('/content/drive/MyDrive/ml/models/cnnpre_4l_2.h5')
# evaluate the model
_, train_acc = saved_model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = saved_model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))



#4 cnn layers
model = models.Sequential()
model.add(layers.Conv2D(256, (3,3), input_shape = (32, 32, 3), strides=1, use_bias=True, data_format="channels_last", activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size = (2,2)))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(256,(3,3), activation = 'relu'))
model.add(layers.Conv2D(256,(3,3), activation = 'relu'))
model.add(layers.Conv2D(128,(3,3), activation = 'relu'))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
model.add(layers.Dropout(0.05))
model.add(layers.Conv2D(64,(3,3), activation = 'relu'))
model.add(layers.Dropout(0.1))
model.add(layers.Flatten())
model.add(layers.Dense(9,activation = 'softmax'))
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = 'adam',
              metrics = ['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=15)
mc = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/6cnn.h5', monitor='val_accuracy', mode='max', verbose=2, save_best_only=True)

history = model.fit(traindata, trainlabels, validation_data=(testdata, testlabels),  epochs=400, verbose=2, callbacks=[es, mc])

from tensorflow.keras.models import load_model
saved_model = load_model('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/6cnn.h5')
# evaluate the model
model.save('/content/drive/MyDrive/Colab Notebooks/Garbage model/models/6cnn.h5')
_, train_acc = model.evaluate(traindata, trainlabels, verbose=2)
_, test_acc = model.evaluate(testdata, testlabels, verbose=2)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))